{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e676550",
   "metadata": {},
   "source": [
    "# Next Word Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99feae23",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc166f",
   "metadata": {},
   "source": [
    "   We often seen while browsing internet, when we are searching for something browser will automatically recommend set of words related to the topic before we type.preloaded data is also stored in the keyboard function of our smartphones to predict the next word correctly.\n",
    "   \n",
    "   This project is useful in various applications, including autocomplete suggestions in search engines, text messaging, and improving the efficiency of virtual assistants and chatbots by making their responses more contextually relevant and coherent.\n",
    "   \n",
    "   I have used Tensorflow and Keras library in Python for next word prediction model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fc4a3",
   "metadata": {},
   "source": [
    "## GitHub Link -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1a612",
   "metadata": {},
   "source": [
    "https://github.com/PrathameshKhochare/-LGMVIP--DataScience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30819ca",
   "metadata": {},
   "source": [
    "# ***Let's Begin !***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082301c",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e7eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.13.0\n",
      "  Downloading tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl (276.5 MB)\n",
      "     -------------------------------------- 276.5/276.5 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.24.1-cp310-abi3-win_amd64.whl (430 kB)\n",
      "     -------------------------------------- 430.4/430.4 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (65.6.3)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "     ------------------------------------ 440.8/440.8 kB 492.1 kB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 24.4/24.4 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.4.0)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.23.5)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 4.3/4.3 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     -------------------------------------- 65.5/65.5 kB 442.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.8/181.8 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.14)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.57.0 keras-2.13.1 libclang-16.0.6 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.1 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n",
      "Requirement already satisfied: ipython-autotime in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: ipython in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython-autotime) (8.10.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (3.0.36)\n",
      "Requirement already satisfied: stack-data in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (5.7.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.1.6)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (2.11.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from ipython->ipython-autotime) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython->ipython-autotime) (0.2.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.2.2)\n",
      "Requirement already satisfied: executing in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (2.0.5)\n",
      "Requirement already satisfied: six in c:\\users\\prathamesh\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython->ipython-autotime) (1.16.0)\n",
      "time: 0 ns (started: 2023-08-20 11:56:37 +05:30)\n"
     ]
    }
   ],
   "source": [
    "## Data Maipulation Libraries\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Library of warnings would assist in ignoring warnings issued\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "import warnings;warnings.simplefilter('ignore')\n",
    " \n",
    "# Importing module to see execution time in mili seconds\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee866ae6",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2047618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e5a720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-08-20 12:15:43 +05:30)\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "file = open(\"C:\\\\Users\\\\Prathamesh\\\\Downloads\\\\1661-0.txt\", \"r\", encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dad2ce",
   "metadata": {},
   "source": [
    "### Data Preproseccing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dffc454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14 s (started: 2023-08-20 12:15:45 +05:30)\n"
     ]
    }
   ],
   "source": [
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    " \n",
    "#replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
    " \n",
    "#remove unnecessary spaces \n",
    "data = data.split()\n",
    "data = ' '.join(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "931045f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Release Date: November 29, 2002 [EBook #1661] Last Updated: May 20, 2019 Language: English Character set en\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-08-20 12:16:35 +05:30)\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adc21d",
   "metadata": {},
   "source": [
    "#### Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "814f9b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2023-08-20 12:16:38 +05:30)\n"
     ]
    }
   ],
   "source": [
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "458e31ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"project gutenberg's the adventures of sherlock holmes, by arthur conan doyle this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. you may copy it, give it away or re-use it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.net title: the adventures of sherlock holmes author: arthur conan doyle release date: november 29, 2002 [ebook #1661] last updated: may 20, 2019 language: english character set en\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-08-20 12:16:41 +05:30)\n"
     ]
    }
   ],
   "source": [
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b13b1",
   "metadata": {},
   "source": [
    "#### Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab48c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 62 ms (started: 2023-08-20 12:16:49 +05:30)\n"
     ]
    }
   ],
   "source": [
    "# removing punctuations\n",
    "import string\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "data = data.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea6d088d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'project gutenbergs the adventures of sherlock holmes by arthur conan doyle this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at wwwgutenbergnet title the adventures of sherlock holmes author arthur conan doyle release date november 29 2002 ebook 1661 last updated may 20 2019 language english character set encoding utf8  star'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-08-20 12:16:51 +05:30)\n"
     ]
    }
   ],
   "source": [
    "# check for update\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0484c4",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Downloading needed libraries\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenization\n",
    "cluster_df['cluster_data'] = cluster_df['cluster_data'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Checking the observation after manipulation\n",
    "cluster_df['cluster_data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "650c9cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 141 ms (started: 2023-08-20 12:22:03 +05:30)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f8f81b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-08-20 12:22:13 +05:30)\n"
     ]
    }
   ],
   "source": [
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "164d0470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[136, 4694, 1, 957, 4, 122, 33, 45, 536, 2112, 2113, 27, 958, 14, 22]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms (started: 2023-08-20 12:23:54 +05:30)\n"
     ]
    }
   ],
   "source": [
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8e31267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107583"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-08-20 12:24:07 +05:30)\n"
     ]
    }
   ],
   "source": [
    "len(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "401b8f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'i': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'a': 6,\n",
       " 'in': 7,\n",
       " 'that': 8,\n",
       " 'it': 9,\n",
       " 'you': 10,\n",
       " 'he': 11,\n",
       " 'was': 12,\n",
       " 'his': 13,\n",
       " 'is': 14,\n",
       " 'my': 15,\n",
       " 'have': 16,\n",
       " 'with': 17,\n",
       " 'as': 18,\n",
       " 'had': 19,\n",
       " 'at': 20,\n",
       " 'which': 21,\n",
       " 'for': 22,\n",
       " 'not': 23,\n",
       " 'but': 24,\n",
       " 'be': 25,\n",
       " 'me': 26,\n",
       " 'this': 27,\n",
       " 'we': 28,\n",
       " 'from': 29,\n",
       " 'there': 30,\n",
       " 'said': 31,\n",
       " 'upon': 32,\n",
       " 'holmes': 33,\n",
       " 'so': 34,\n",
       " 'him': 35,\n",
       " 'her': 36,\n",
       " 'she': 37,\n",
       " 'your': 38,\n",
       " 'all': 39,\n",
       " 'very': 40,\n",
       " 'been': 41,\n",
       " 'on': 42,\n",
       " 'no': 43,\n",
       " 'what': 44,\n",
       " 'by': 45,\n",
       " 'one': 46,\n",
       " 'are': 47,\n",
       " 'then': 48,\n",
       " 'were': 49,\n",
       " 'an': 50,\n",
       " 'would': 51,\n",
       " 'when': 52,\n",
       " 'out': 53,\n",
       " 'up': 54,\n",
       " 'do': 55,\n",
       " 'could': 56,\n",
       " 'has': 57,\n",
       " 'man': 58,\n",
       " 'if': 59,\n",
       " 'into': 60,\n",
       " 'or': 61,\n",
       " 'who': 62,\n",
       " 'mr': 63,\n",
       " 'little': 64,\n",
       " 'will': 65,\n",
       " 'some': 66,\n",
       " 'see': 67,\n",
       " 'down': 68,\n",
       " 'now': 69,\n",
       " 'our': 70,\n",
       " 'may': 71,\n",
       " 'should': 72,\n",
       " 'they': 73,\n",
       " 'us': 74,\n",
       " 'am': 75,\n",
       " 'over': 76,\n",
       " 'can': 77,\n",
       " 'more': 78,\n",
       " 'about': 79,\n",
       " 'think': 80,\n",
       " 'well': 81,\n",
       " 'must': 82,\n",
       " 'shall': 83,\n",
       " 'know': 84,\n",
       " 'before': 85,\n",
       " 'any': 86,\n",
       " 'only': 87,\n",
       " 'other': 88,\n",
       " 'than': 89,\n",
       " 'come': 90,\n",
       " 'did': 91,\n",
       " 'time': 92,\n",
       " 'came': 93,\n",
       " 'them': 94,\n",
       " 'two': 95,\n",
       " 'how': 96,\n",
       " 'door': 97,\n",
       " 'back': 98,\n",
       " 'room': 99,\n",
       " 'face': 100,\n",
       " 'here': 101,\n",
       " 'might': 102,\n",
       " 'just': 103,\n",
       " 'matter': 104,\n",
       " 'much': 105,\n",
       " 'where': 106,\n",
       " 'such': 107,\n",
       " 'way': 108,\n",
       " 'heard': 109,\n",
       " 'yes': 110,\n",
       " 'hand': 111,\n",
       " 'case': 112,\n",
       " 'house': 113,\n",
       " 'found': 114,\n",
       " 'however': 115,\n",
       " 'made': 116,\n",
       " 'nothing': 117,\n",
       " 'away': 118,\n",
       " 'good': 119,\n",
       " 'never': 120,\n",
       " 'quite': 121,\n",
       " 'sherlock': 122,\n",
       " 'own': 123,\n",
       " 'after': 124,\n",
       " 'morning': 125,\n",
       " 'right': 126,\n",
       " 'their': 127,\n",
       " 'go': 128,\n",
       " 'last': 129,\n",
       " 'tell': 130,\n",
       " 'through': 131,\n",
       " 'like': 132,\n",
       " 'say': 133,\n",
       " 'most': 134,\n",
       " 'its': 135,\n",
       " 'project': 136,\n",
       " 'saw': 137,\n",
       " 'long': 138,\n",
       " 'asked': 139,\n",
       " 'yet': 140,\n",
       " 'eyes': 141,\n",
       " 'took': 142,\n",
       " 'miss': 143,\n",
       " 'first': 144,\n",
       " 'work': 145,\n",
       " 'once': 146,\n",
       " 'day': 147,\n",
       " 'left': 148,\n",
       " 'these': 149,\n",
       " 'every': 150,\n",
       " 'street': 151,\n",
       " 'watson': 152,\n",
       " 'round': 153,\n",
       " 'without': 154,\n",
       " 'small': 155,\n",
       " 'young': 156,\n",
       " 'st': 157,\n",
       " 'too': 158,\n",
       " 'side': 159,\n",
       " 'find': 160,\n",
       " 'take': 161,\n",
       " 'still': 162,\n",
       " 'thought': 163,\n",
       " 'few': 164,\n",
       " 'myself': 165,\n",
       " 'make': 166,\n",
       " 'until': 167,\n",
       " 'oh': 168,\n",
       " 'business': 169,\n",
       " 'off': 170,\n",
       " 'himself': 171,\n",
       " 'light': 172,\n",
       " 'night': 173,\n",
       " 'sir': 174,\n",
       " 'even': 175,\n",
       " 'hands': 176,\n",
       " 'father': 177,\n",
       " 'seen': 178,\n",
       " 'look': 179,\n",
       " 'old': 180,\n",
       " 'ever': 181,\n",
       " 'window': 182,\n",
       " 'friend': 183,\n",
       " 'lady': 184,\n",
       " 'let': 185,\n",
       " 'cried': 186,\n",
       " 'three': 187,\n",
       " 'went': 188,\n",
       " 'seemed': 189,\n",
       " 'having': 190,\n",
       " 'while': 191,\n",
       " 'those': 192,\n",
       " 'head': 193,\n",
       " 'put': 194,\n",
       " 'done': 195,\n",
       " 'why': 196,\n",
       " 'again': 197,\n",
       " 'give': 198,\n",
       " 'remarked': 199,\n",
       " 'rather': 200,\n",
       " 'years': 201,\n",
       " 'something': 202,\n",
       " 'doubt': 203,\n",
       " 'name': 204,\n",
       " 'indeed': 205,\n",
       " 'though': 206,\n",
       " 'open': 207,\n",
       " 'get': 208,\n",
       " 'course': 209,\n",
       " 'great': 210,\n",
       " 'perhaps': 211,\n",
       " 'between': 212,\n",
       " 'always': 213,\n",
       " 'woman': 214,\n",
       " 'mind': 215,\n",
       " 'knew': 216,\n",
       " 'enough': 217,\n",
       " 'end': 218,\n",
       " 'answered': 219,\n",
       " 'chair': 220,\n",
       " 'same': 221,\n",
       " 'gutenbergtm': 222,\n",
       " 'far': 223,\n",
       " 'sat': 224,\n",
       " 'cannot': 225,\n",
       " 'against': 226,\n",
       " 'dear': 227,\n",
       " 'anything': 228,\n",
       " 'got': 229,\n",
       " 'also': 230,\n",
       " 'place': 231,\n",
       " 'looked': 232,\n",
       " 'wife': 233,\n",
       " 'really': 234,\n",
       " 'turned': 235,\n",
       " 'within': 236,\n",
       " 'set': 237,\n",
       " 'behind': 238,\n",
       " 'told': 239,\n",
       " 'hardly': 240,\n",
       " 'better': 241,\n",
       " 'front': 242,\n",
       " 'brought': 243,\n",
       " 'black': 244,\n",
       " 'help': 245,\n",
       " 'understand': 246,\n",
       " 'possible': 247,\n",
       " 'under': 248,\n",
       " 'both': 249,\n",
       " 'life': 250,\n",
       " 'leave': 251,\n",
       " 'already': 252,\n",
       " 'suddenly': 253,\n",
       " 'home': 254,\n",
       " 'thing': 255,\n",
       " 'strange': 256,\n",
       " 'gave': 257,\n",
       " 'words': 258,\n",
       " 'son': 259,\n",
       " 'whole': 260,\n",
       " 'being': 261,\n",
       " 'money': 262,\n",
       " 'papers': 263,\n",
       " 'days': 264,\n",
       " 'use': 265,\n",
       " 'fire': 266,\n",
       " 'clear': 267,\n",
       " 'wish': 268,\n",
       " 'many': 269,\n",
       " 'point': 270,\n",
       " 'table': 271,\n",
       " 'call': 272,\n",
       " 'hat': 273,\n",
       " 'sure': 274,\n",
       " 'whether': 275,\n",
       " 'looking': 276,\n",
       " 'mrs': 277,\n",
       " 'hair': 278,\n",
       " 'yourself': 279,\n",
       " 'certainly': 280,\n",
       " 'gentleman': 281,\n",
       " 'pray': 282,\n",
       " '‘i': 283,\n",
       " 'lay': 284,\n",
       " 'gone': 285,\n",
       " 'baker': 286,\n",
       " 'police': 287,\n",
       " 'passed': 288,\n",
       " 'large': 289,\n",
       " 'does': 290,\n",
       " 'another': 291,\n",
       " 'met': 292,\n",
       " 'word': 293,\n",
       " 'paper': 294,\n",
       " 'whom': 295,\n",
       " 'minutes': 296,\n",
       " 'half': 297,\n",
       " 'since': 298,\n",
       " 'read': 299,\n",
       " 'soon': 300,\n",
       " 'lord': 301,\n",
       " 'london': 302,\n",
       " 'less': 303,\n",
       " 'during': 304,\n",
       " 'interest': 305,\n",
       " 'bed': 306,\n",
       " 'simon': 307,\n",
       " 'save': 308,\n",
       " 'dark': 309,\n",
       " 'across': 310,\n",
       " 'men': 311,\n",
       " 'lestrade': 312,\n",
       " 'five': 313,\n",
       " 'returned': 314,\n",
       " 'part': 315,\n",
       " 'evening': 316,\n",
       " '£': 317,\n",
       " 'among': 318,\n",
       " 'stood': 319,\n",
       " 'together': 320,\n",
       " 'believe': 321,\n",
       " 'facts': 322,\n",
       " 'question': 323,\n",
       " 'opened': 324,\n",
       " 'least': 325,\n",
       " 'story': 326,\n",
       " 'either': 327,\n",
       " 'moment': 328,\n",
       " 'doctor': 329,\n",
       " 'ask': 330,\n",
       " 'strong': 331,\n",
       " 'each': 332,\n",
       " 'new': 333,\n",
       " 'country': 334,\n",
       " 'given': 335,\n",
       " 'hear': 336,\n",
       " 'o’clock': 337,\n",
       " 'entered': 338,\n",
       " 'rushed': 339,\n",
       " 'fellow': 340,\n",
       " 'mccarthy': 341,\n",
       " 'rucastle': 342,\n",
       " 'gutenberg': 343,\n",
       " 'felt': 344,\n",
       " 'crime': 345,\n",
       " 'singular': 346,\n",
       " 'corner': 347,\n",
       " 'else': 348,\n",
       " 'none': 349,\n",
       " 'laid': 350,\n",
       " 'hope': 351,\n",
       " 'waiting': 352,\n",
       " 'used': 353,\n",
       " 'works': 354,\n",
       " 'note': 355,\n",
       " 'hour': 356,\n",
       " 'sound': 357,\n",
       " 'best': 358,\n",
       " 'able': 359,\n",
       " 'became': 360,\n",
       " 'four': 361,\n",
       " 'road': 362,\n",
       " 'near': 363,\n",
       " 'several': 364,\n",
       " 'forward': 365,\n",
       " 'going': 366,\n",
       " '‘you': 367,\n",
       " 'companion': 368,\n",
       " 'dr': 369,\n",
       " 'client': 370,\n",
       " 'turn': 371,\n",
       " 'instant': 372,\n",
       " 'anyone': 373,\n",
       " 'things': 374,\n",
       " 'threw': 375,\n",
       " 'true': 376,\n",
       " 'heavy': 377,\n",
       " 'ten': 378,\n",
       " 'letter': 379,\n",
       " 'obvious': 380,\n",
       " 'stone': 381,\n",
       " 'inspector': 382,\n",
       " 'family': 383,\n",
       " 'manner': 384,\n",
       " 'seven': 385,\n",
       " 'imagine': 386,\n",
       " 'don’t': 387,\n",
       " 'want': 388,\n",
       " 'feet': 389,\n",
       " 'walked': 390,\n",
       " 'it’s': 391,\n",
       " 'floor': 392,\n",
       " 'people': 393,\n",
       " 'fear': 394,\n",
       " 'seems': 395,\n",
       " 'cry': 396,\n",
       " 'coming': 397,\n",
       " 'ah': 398,\n",
       " 'red': 399,\n",
       " 'ran': 400,\n",
       " 'keep': 401,\n",
       " 'full': 402,\n",
       " 'cut': 403,\n",
       " 'spoke': 404,\n",
       " 'rooms': 405,\n",
       " 'ago': 406,\n",
       " 'address': 407,\n",
       " 'death': 408,\n",
       " 'lost': 409,\n",
       " 'outside': 410,\n",
       " 'appeared': 411,\n",
       " 'alone': 412,\n",
       " 'present': 413,\n",
       " 'taken': 414,\n",
       " 'absolutely': 415,\n",
       " 'air': 416,\n",
       " 'hard': 417,\n",
       " 'above': 418,\n",
       " 'goose': 419,\n",
       " 'colonel': 420,\n",
       " 'electronic': 421,\n",
       " 'coronet': 422,\n",
       " 'late': 423,\n",
       " 'marriage': 424,\n",
       " 'visitor': 425,\n",
       " 'person': 426,\n",
       " 'called': 427,\n",
       " 'white': 428,\n",
       " 'letters': 429,\n",
       " 'second': 430,\n",
       " 'cab': 431,\n",
       " 'drove': 432,\n",
       " 'remember': 433,\n",
       " 'reason': 434,\n",
       " 'office': 435,\n",
       " 'sister': 436,\n",
       " 'led': 437,\n",
       " 'followed': 438,\n",
       " 'dress': 439,\n",
       " 'king': 440,\n",
       " 'photograph': 441,\n",
       " 'married': 442,\n",
       " 'beside': 443,\n",
       " 'windows': 444,\n",
       " 'began': 445,\n",
       " 'mine': 446,\n",
       " 'held': 447,\n",
       " 'fact': 448,\n",
       " 'square': 449,\n",
       " 'states': 450,\n",
       " 'god': 451,\n",
       " 'terms': 452,\n",
       " 'blue': 453,\n",
       " 'attention': 454,\n",
       " 'week': 455,\n",
       " 'eye': 456,\n",
       " 'six': 457,\n",
       " 'steps': 458,\n",
       " 'rose': 459,\n",
       " 'year': 460,\n",
       " 'likely': 461,\n",
       " 'started': 462,\n",
       " 'chance': 463,\n",
       " 'struck': 464,\n",
       " 'heart': 465,\n",
       " 'silence': 466,\n",
       " 'thank': 467,\n",
       " 'known': 468,\n",
       " 'holder': 469,\n",
       " 'nature': 470,\n",
       " 'because': 471,\n",
       " 'written': 472,\n",
       " 'glancing': 473,\n",
       " 'deep': 474,\n",
       " 'past': 475,\n",
       " 'cases': 476,\n",
       " 'sprang': 477,\n",
       " 'bring': 478,\n",
       " 'next': 479,\n",
       " 'quick': 480,\n",
       " 'lane': 481,\n",
       " 'whose': 482,\n",
       " 'standing': 483,\n",
       " 'dressed': 484,\n",
       " 'easy': 485,\n",
       " 'grey': 486,\n",
       " 'considerable': 487,\n",
       " 'ready': 488,\n",
       " 'happened': 489,\n",
       " 'idea': 490,\n",
       " 'lamp': 491,\n",
       " 'clair': 492,\n",
       " 'mystery': 493,\n",
       " 'position': 494,\n",
       " 'girl': 495,\n",
       " 'observed': 496,\n",
       " 'tonight': 497,\n",
       " 'step': 498,\n",
       " 'passage': 499,\n",
       " 'carried': 500,\n",
       " 'glanced': 501,\n",
       " 'ground': 502,\n",
       " 'daughter': 503,\n",
       " 'pocket': 504,\n",
       " 'hurried': 505,\n",
       " 'sort': 506,\n",
       " 'cause': 507,\n",
       " 'john': 508,\n",
       " 'poor': 509,\n",
       " 'drawn': 510,\n",
       " 'need': 511,\n",
       " 'return': 512,\n",
       " 'closed': 513,\n",
       " 'city': 514,\n",
       " 'town': 515,\n",
       " 'short': 516,\n",
       " 'occurred': 517,\n",
       " 'public': 518,\n",
       " 'seeing': 519,\n",
       " 'foundation': 520,\n",
       " 'cold': 521,\n",
       " 'problem': 522,\n",
       " 'mary': 523,\n",
       " 'order': 524,\n",
       " 'means': 525,\n",
       " 'interesting': 526,\n",
       " 'doing': 527,\n",
       " 'station': 528,\n",
       " 'quiet': 529,\n",
       " 'husband': 530,\n",
       " 'afraid': 531,\n",
       " 'sight': 532,\n",
       " 'points': 533,\n",
       " 'thin': 534,\n",
       " 'hosmer': 535,\n",
       " 'arthur': 536,\n",
       " 'almost': 537,\n",
       " 'redheaded': 538,\n",
       " 'account': 539,\n",
       " 'shown': 540,\n",
       " 'ha': 541,\n",
       " 'sent': 542,\n",
       " 'caught': 543,\n",
       " 'speak': 544,\n",
       " 'laughed': 545,\n",
       " 'remarkable': 546,\n",
       " 'quietly': 547,\n",
       " 'important': 548,\n",
       " 'others': 549,\n",
       " 'towards': 550,\n",
       " 'mean': 551,\n",
       " 'turner': 552,\n",
       " 'fresh': 553,\n",
       " 'ourselves': 554,\n",
       " 'carriage': 555,\n",
       " 'showed': 556,\n",
       " 'train': 557,\n",
       " 'safe': 558,\n",
       " 'body': 559,\n",
       " 'experience': 560,\n",
       " 'everything': 561,\n",
       " 'opinion': 562,\n",
       " 'bird': 563,\n",
       " 'talk': 564,\n",
       " 'danger': 565,\n",
       " 'mother': 566,\n",
       " 'character': 567,\n",
       " 'adventure': 568,\n",
       " 'placed': 569,\n",
       " 'extraordinary': 570,\n",
       " 'finally': 571,\n",
       " 'figure': 572,\n",
       " 'fashion': 573,\n",
       " 'observe': 574,\n",
       " 'clothes': 575,\n",
       " 'someone': 576,\n",
       " 'hall': 577,\n",
       " 'peculiar': 578,\n",
       " 'voice': 579,\n",
       " 'slowly': 580,\n",
       " 'sign': 581,\n",
       " 'line': 582,\n",
       " 'reached': 583,\n",
       " 'details': 584,\n",
       " 'maid': 585,\n",
       " 'hours': 586,\n",
       " 'direction': 587,\n",
       " 'kind': 588,\n",
       " 'rest': 589,\n",
       " 'advertisement': 590,\n",
       " 'effect': 591,\n",
       " 'colour': 592,\n",
       " 'advice': 593,\n",
       " 'stepfather': 594,\n",
       " 'windibank': 595,\n",
       " 'pool': 596,\n",
       " 'frank': 597,\n",
       " 'charge': 598,\n",
       " 'stoner': 599,\n",
       " 'copyright': 600,\n",
       " 'love': 601,\n",
       " 'remained': 602,\n",
       " 'glad': 603,\n",
       " 'itself': 604,\n",
       " 'appears': 605,\n",
       " 'simple': 606,\n",
       " 'yours': 607,\n",
       " 'matters': 608,\n",
       " 'received': 609,\n",
       " 'hundred': 610,\n",
       " 'england': 611,\n",
       " 'coat': 612,\n",
       " 'single': 613,\n",
       " 'high': 614,\n",
       " 'serious': 615,\n",
       " 'news': 616,\n",
       " 'close': 617,\n",
       " 'bedroom': 618,\n",
       " 'suppose': 619,\n",
       " 'listened': 620,\n",
       " 'wait': 621,\n",
       " '‘and': 622,\n",
       " 'law': 623,\n",
       " 'nor': 624,\n",
       " 'excellent': 625,\n",
       " 'meet': 626,\n",
       " 'afterwards': 627,\n",
       " 'entirely': 628,\n",
       " 'holmes’': 629,\n",
       " 'dead': 630,\n",
       " 'fell': 631,\n",
       " 'wilson': 632,\n",
       " 'lips': 633,\n",
       " 'low': 634,\n",
       " 'locked': 635,\n",
       " 'happy': 636,\n",
       " 'pipe': 637,\n",
       " 'along': 638,\n",
       " 'silent': 639,\n",
       " 'i’ll': 640,\n",
       " 'angel': 641,\n",
       " 'james': 642,\n",
       " 'neville': 643,\n",
       " 'agreement': 644,\n",
       " 'license': 645,\n",
       " 'dreadful': 646,\n",
       " 'laughing': 647,\n",
       " 'times': 648,\n",
       " 'glance': 649,\n",
       " 'sit': 650,\n",
       " 'state': 651,\n",
       " 'information': 652,\n",
       " 'secret': 653,\n",
       " 'tomorrow': 654,\n",
       " 'certain': 655,\n",
       " 'garden': 656,\n",
       " 'child': 657,\n",
       " 'pulled': 658,\n",
       " 'won’t': 659,\n",
       " 'drive': 660,\n",
       " 'drew': 661,\n",
       " 'making': 662,\n",
       " 'change': 663,\n",
       " 'seem': 664,\n",
       " 'weeks': 665,\n",
       " 'instantly': 666,\n",
       " 'surprised': 667,\n",
       " 'feel': 668,\n",
       " 'wedding': 669,\n",
       " 'breakfast': 670,\n",
       " 'evidence': 671,\n",
       " 'miles': 672,\n",
       " 'innocent': 673,\n",
       " 'feeling': 674,\n",
       " 'hunter': 675,\n",
       " 'start': 676,\n",
       " 'league': 677,\n",
       " 'boscombe': 678,\n",
       " 'thumb': 679,\n",
       " 'irene': 680,\n",
       " 'adler': 681,\n",
       " 'twice': 682,\n",
       " 'chamber': 683,\n",
       " 'kindly': 684,\n",
       " 'show': 685,\n",
       " 'explain': 686,\n",
       " 'importance': 687,\n",
       " 'examined': 688,\n",
       " 'brown': 689,\n",
       " 'boy': 690,\n",
       " 'shoulders': 691,\n",
       " 'boots': 692,\n",
       " 'impression': 693,\n",
       " 'appearance': 694,\n",
       " 'trust': 695,\n",
       " 'confess': 696,\n",
       " 'surprise': 697,\n",
       " 'majesty': 698,\n",
       " 'sitting': 699,\n",
       " 'gold': 700,\n",
       " 'lock': 701,\n",
       " 'arrived': 702,\n",
       " 'dropped': 703,\n",
       " 'whispered': 704,\n",
       " 'centre': 705,\n",
       " 'determined': 706,\n",
       " 'key': 707,\n",
       " 'turning': 708,\n",
       " 'company': 709,\n",
       " 'events': 710,\n",
       " 'shook': 711,\n",
       " 'bright': 712,\n",
       " 'third': 713,\n",
       " 'smiling': 714,\n",
       " 'sudden': 715,\n",
       " 'box': 716,\n",
       " 'hatherley': 717,\n",
       " 'cleared': 718,\n",
       " 'k': 719,\n",
       " 'united': 720,\n",
       " 'following': 721,\n",
       " 'armchair': 722,\n",
       " 'fancy': 723,\n",
       " 'walk': 724,\n",
       " 'caused': 725,\n",
       " 'therefore': 726,\n",
       " 'comes': 727,\n",
       " 'sharp': 728,\n",
       " 'excuse': 729,\n",
       " 'follow': 730,\n",
       " 'pay': 731,\n",
       " 'afternoon': 732,\n",
       " 'sittingroom': 733,\n",
       " 'view': 734,\n",
       " 'deal': 735,\n",
       " 'object': 736,\n",
       " 'probably': 737,\n",
       " 'paid': 738,\n",
       " 'witness': 739,\n",
       " 'nearly': 740,\n",
       " 'whatever': 741,\n",
       " 'taking': 742,\n",
       " 'number': 743,\n",
       " 'neither': 744,\n",
       " 'waited': 745,\n",
       " 'engaged': 746,\n",
       " 'today': 747,\n",
       " 'care': 748,\n",
       " 'slight': 749,\n",
       " 'live': 750,\n",
       " 'says': 751,\n",
       " 'foot': 752,\n",
       " 'didn’t': 753,\n",
       " 'answer': 754,\n",
       " 'wooden': 755,\n",
       " 'water': 756,\n",
       " 'clearly': 757,\n",
       " 'yard': 758,\n",
       " 'force': 759,\n",
       " 'unless': 760,\n",
       " 'reading': 761,\n",
       " 'wrong': 762,\n",
       " 'coroner': 763,\n",
       " 'terrible': 764,\n",
       " 'openshaw': 765,\n",
       " 'geese': 766,\n",
       " 'toller': 767,\n",
       " 'donations': 768,\n",
       " 'copy': 769,\n",
       " 'noble': 770,\n",
       " 'world': 771,\n",
       " 'society': 772,\n",
       " 'official': 773,\n",
       " 'pass': 774,\n",
       " 'swiftly': 775,\n",
       " 'bell': 776,\n",
       " 'lying': 777,\n",
       " 'writing': 778,\n",
       " 'wrote': 779,\n",
       " 'german': 780,\n",
       " 'scene': 781,\n",
       " 'continued': 782,\n",
       " 'bad': 783,\n",
       " 'pushed': 784,\n",
       " 'difficult': 785,\n",
       " 'private': 786,\n",
       " 'result': 787,\n",
       " 'send': 788,\n",
       " 'monday': 789,\n",
       " 'lodge': 790,\n",
       " 'features': 791,\n",
       " 'wall': 792,\n",
       " 'arms': 793,\n",
       " 'church': 794,\n",
       " '‘the': 795,\n",
       " 'run': 796,\n",
       " 'different': 797,\n",
       " 'broke': 798,\n",
       " 'blood': 799,\n",
       " 'later': 800,\n",
       " 'walking': 801,\n",
       " 'age': 802,\n",
       " 'court': 803,\n",
       " 'affair': 804,\n",
       " 'assistant': 805,\n",
       " 'clay': 806,\n",
       " 'claim': 807,\n",
       " 'amid': 808,\n",
       " 'early': 809,\n",
       " 'pale': 810,\n",
       " 'man’s': 811,\n",
       " 'friends': 812,\n",
       " 'hotel': 813,\n",
       " 'professional': 814,\n",
       " 'free': 815,\n",
       " 'paragraph': 816,\n",
       " 'truth': 817,\n",
       " 'bent': 818,\n",
       " 'envelope': 819,\n",
       " 'den': 820,\n",
       " 'roylott': 821,\n",
       " 'ventilator': 822,\n",
       " 'literary': 823,\n",
       " 'scandal': 824,\n",
       " 'copper': 825,\n",
       " 'machine': 826,\n",
       " 'form': 827,\n",
       " 'signs': 828,\n",
       " 'lit': 829,\n",
       " 'deduce': 830,\n",
       " 'inside': 831,\n",
       " 'example': 832,\n",
       " 'often': 833,\n",
       " 'thick': 834,\n",
       " 'eight': 835,\n",
       " 'stairs': 836,\n",
       " 'broad': 837,\n",
       " 'absolute': 838,\n",
       " 'promise': 839,\n",
       " 'purpose': 840,\n",
       " 'subject': 841,\n",
       " 'tried': 842,\n",
       " 'situation': 843,\n",
       " 'expected': 844,\n",
       " 'glass': 845,\n",
       " 'evidently': 846,\n",
       " 'shot': 847,\n",
       " 'running': 848,\n",
       " 'raise': 849,\n",
       " 'carry': 850,\n",
       " 'besides': 851,\n",
       " 'he’s': 852,\n",
       " 'shoulder': 853,\n",
       " 'months': 854,\n",
       " 'slipped': 855,\n",
       " 'ring': 856,\n",
       " 'finger': 857,\n",
       " 'impossible': 858,\n",
       " 'heavily': 859,\n",
       " 'ross': 860,\n",
       " 'common': 861,\n",
       " 'cellar': 862,\n",
       " 'died': 863,\n",
       " 'sum': 864,\n",
       " 'knowledge': 865,\n",
       " 'bank': 866,\n",
       " 'presence': 867,\n",
       " 'darkness': 868,\n",
       " 'yellow': 869,\n",
       " 'edge': 870,\n",
       " 'clue': 871,\n",
       " 'mad': 872,\n",
       " 'moran': 873,\n",
       " 'break': 874,\n",
       " 'horrible': 875,\n",
       " 'horner': 876,\n",
       " 'fee': 877,\n",
       " 'gems': 878,\n",
       " 'archive': 879,\n",
       " 'bohemia': 880,\n",
       " 'pips': 881,\n",
       " 'lip': 882,\n",
       " 'band': 883,\n",
       " 'beeches': 884,\n",
       " 'results': 885,\n",
       " 'complete': 886,\n",
       " 'deeply': 887,\n",
       " 'practice': 888,\n",
       " 'associated': 889,\n",
       " 'tall': 890,\n",
       " 'spoken': 891,\n",
       " 'lived': 892,\n",
       " 'can’t': 893,\n",
       " 'double': 894,\n",
       " 'throwing': 895,\n",
       " 'sheet': 896,\n",
       " 'houses': 897,\n",
       " 'precisely': 898,\n",
       " 'pair': 899,\n",
       " 'there’s': 900,\n",
       " 'chin': 901,\n",
       " 'marked': 902,\n",
       " 'honour': 903,\n",
       " 'passing': 904,\n",
       " 'power': 905,\n",
       " 'visit': 906,\n",
       " 'pretty': 907,\n",
       " 'inquiry': 908,\n",
       " 'investigation': 909,\n",
       " 'twenty': 910,\n",
       " 'reach': 911,\n",
       " 'streets': 912,\n",
       " 'search': 913,\n",
       " 'action': 914,\n",
       " 'knows': 915,\n",
       " 'blow': 916,\n",
       " 'draw': 917,\n",
       " 'perfectly': 918,\n",
       " 'wonder': 919,\n",
       " 'yesterday': 920,\n",
       " 'future': 921,\n",
       " 'broken': 922,\n",
       " 'wished': 923,\n",
       " 'narrative': 924,\n",
       " 'learn': 925,\n",
       " '‘oh': 926,\n",
       " 'ways': 927,\n",
       " 'real': 928,\n",
       " 'west': 929,\n",
       " 'statement': 930,\n",
       " 'human': 931,\n",
       " 'building': 932,\n",
       " 'usual': 933,\n",
       " 'beg': 934,\n",
       " 'lawn': 935,\n",
       " 'shining': 936,\n",
       " 'merryweather': 937,\n",
       " 'iron': 938,\n",
       " 'lens': 939,\n",
       " 'kept': 940,\n",
       " 'weary': 941,\n",
       " 'worn': 942,\n",
       " 'alive': 943,\n",
       " 'uncle': 944,\n",
       " 'firm': 945,\n",
       " 'traces': 946,\n",
       " 'missing': 947,\n",
       " 'looks': 948,\n",
       " 'trouble': 949,\n",
       " 'sometimes': 950,\n",
       " 'grew': 951,\n",
       " 'trademark': 952,\n",
       " 'lascar': 953,\n",
       " 'bradstreet': 954,\n",
       " 'permission': 955,\n",
       " 'forth': 956,\n",
       " 'adventures': 957,\n",
       " 'ebook': 958,\n",
       " 'orange': 959,\n",
       " 'reasoning': 960,\n",
       " 'throw': 961,\n",
       " 'keen': 962,\n",
       " 'beyond': 963,\n",
       " 'returning': 964,\n",
       " 'easily': 965,\n",
       " 'interested': 966,\n",
       " 'data': 967,\n",
       " 'wanted': 968,\n",
       " 'rich': 969,\n",
       " 'raised': 970,\n",
       " 'seat': 971,\n",
       " 'influence': 972,\n",
       " 'acquaintance': 973,\n",
       " 'opening': 974,\n",
       " 'beautiful': 975,\n",
       " 'briony': 976,\n",
       " 'neighbourhood': 977,\n",
       " 'general': 978,\n",
       " 'woman’s': 979,\n",
       " 'fall': 980,\n",
       " 'creature': 981,\n",
       " 'arm': 982,\n",
       " 'precious': 983,\n",
       " 'evil': 984,\n",
       " 'upstairs': 985,\n",
       " 'possibly': 986,\n",
       " 'remark': 987,\n",
       " 'thrust': 988,\n",
       " 'presume': 989,\n",
       " 'couple': 990,\n",
       " 'american': 991,\n",
       " 'sake': 992,\n",
       " 'huge': 993,\n",
       " '‘it': 994,\n",
       " 'exceedingly': 995,\n",
       " '‘no': 996,\n",
       " 'piece': 997,\n",
       " 'lad': 998,\n",
       " 'fingers': 999,\n",
       " 'explanation': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms (started: 2023-08-20 12:24:48 +05:30)\n"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a669a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9178\n",
      "time: 0 ns (started: 2023-08-20 12:27:15 +05:30)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6723d718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 282 ms (started: 2023-08-20 12:29:22 +05:30)\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    " \n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3222ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  107580\n",
      "time: 0 ns (started: 2023-08-20 12:29:53 +05:30)\n"
     ]
    }
   ],
   "source": [
    "print(\"The Length of sequences are: \", len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa307f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 136, 4694,    1,  957],\n",
       "       [4694,    1,  957,    4],\n",
       "       [   1,  957,    4,  122],\n",
       "       [ 957,    4,  122,   33],\n",
       "       [   4,  122,   33,   45],\n",
       "       [ 122,   33,   45,  536],\n",
       "       [  33,   45,  536, 2112],\n",
       "       [  45,  536, 2112, 2113],\n",
       "       [ 536, 2112, 2113,   27],\n",
       "       [2112, 2113,   27,  958]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 94 ms (started: 2023-08-20 12:30:21 +05:30)\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b24d8f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 109 ms (started: 2023-08-20 12:30:58 +05:30)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9410bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[ 136 4694    1]\n",
      " [4694    1  957]\n",
      " [   1  957    4]\n",
      " [ 957    4  122]\n",
      " [   4  122   33]\n",
      " [ 122   33   45]\n",
      " [  33   45  536]\n",
      " [  45  536 2112]\n",
      " [ 536 2112 2113]\n",
      " [2112 2113   27]]\n",
      "Response:  [ 957    4  122   33   45  536 2112 2113   27  958]\n",
      "time: 78 ms (started: 2023-08-20 12:31:21 +05:30)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93fbf8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107580, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-08-20 12:35:43 +05:30)\n"
     ]
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "591017c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 203 ms (started: 2023-08-20 12:31:45 +05:30)\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90b563d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107580, 9178)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-08-20 12:35:56 +05:30)\n"
     ]
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96744030",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71c4dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.92 s (started: 2023-08-20 12:37:31 +05:30)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "286675df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 10)             91780     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9178)              9187178   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22327958 (85.17 MB)\n",
      "Trainable params: 22327958 (85.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "time: 15 ms (started: 2023-08-20 12:37:39 +05:30)\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c64a14",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee5fc14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 5.1946\n",
      "Epoch 1: loss improved from inf to 5.19463, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 1002s 463ms/step - loss: 5.1946\n",
      "Epoch 2/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 4.9519\n",
      "Epoch 2: loss improved from 5.19463 to 4.95193, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 904s 420ms/step - loss: 4.9519\n",
      "Epoch 3/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 4.7505\n",
      "Epoch 3: loss improved from 4.95193 to 4.75046, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 951s 442ms/step - loss: 4.7505\n",
      "Epoch 4/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 4.5579\n",
      "Epoch 4: loss improved from 4.75046 to 4.55795, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 986s 458ms/step - loss: 4.5579\n",
      "Epoch 5/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 4.3670\n",
      "Epoch 5: loss improved from 4.55795 to 4.36696, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 1053s 489ms/step - loss: 4.3670\n",
      "Epoch 6/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 4.1745\n",
      "Epoch 6: loss improved from 4.36696 to 4.17452, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 944s 439ms/step - loss: 4.1745\n",
      "Epoch 7/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 3.9714\n",
      "Epoch 7: loss improved from 4.17452 to 3.97144, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 924s 429ms/step - loss: 3.9714\n",
      "Epoch 8/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 3.7510\n",
      "Epoch 8: loss improved from 3.97144 to 3.75097, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 859s 399ms/step - loss: 3.7510\n",
      "Epoch 9/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 3.4988\n",
      "Epoch 9: loss improved from 3.75097 to 3.49875, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 918s 426ms/step - loss: 3.4988\n",
      "Epoch 10/10\n",
      "2152/2152 [==============================] - ETA: 0s - loss: 3.2277\n",
      "Epoch 10: loss improved from 3.49875 to 3.22766, saving model to next_words.h5\n",
      "2152/2152 [==============================] - 1087s 505ms/step - loss: 3.2277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18f1e18e2c0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2h 40min 34s (started: 2023-08-20 16:09:57 +05:30)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    " \n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X, y, epochs=10, batch_size=50, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c7ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: Produced by an anonymous Project\n",
      "['an', 'anonymous', 'Project']\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "gutenberg\n",
      "Enter your line: The Adventures of\n",
      "['The', 'Adventures', 'of']\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "sherlock\n",
      "Enter your line: I had seen little of Holmes\n",
      "['little', 'of', 'Holmes']\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "lysander\n",
      "Enter your line: The Project Gutenberg eBook of\n",
      "['Gutenberg', 'eBook', 'of']\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "the\n",
      "Enter your line: how can you abuse your own\n",
      "['abuse', 'your', 'own']\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "art\n",
      "Enter your line: He could not help seeing that you were about five times as\n",
      "['five', 'times', 'as']\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "to\n",
      "Enter your line: and her sister\n",
      "['and', 'her', 'sister']\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "was\n",
      "Enter your line: and her sister was\n",
      "['her', 'sister', 'was']\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "returning\n",
      "Enter your line: however, it may all come to\n",
      "['all', 'come', 'to']\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "me\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    " \n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    " \n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "   \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "   \n",
    "  print(predicted_word)\n",
    "  return predicted_word\n",
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "   \n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break\n",
    "   \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "         \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "           \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cca88",
   "metadata": {},
   "source": [
    "## Conclusion -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955041d4",
   "metadata": {},
   "source": [
    "From the above results it is seen that we are correctly predicting the next word by evaluating previous three words.\n",
    "We found almost 90% of time we got correct result as per out dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d22546",
   "metadata": {},
   "source": [
    "# Thanks !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad95bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
